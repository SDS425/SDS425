state.name = c(state.name, "District of Columbia")
state.abb = c(state.abb, "DC")
formals(merge.data.table)$all = TRUE
merge = merge.data.table
CSV_files = grep(".csv", list.files(), value = TRUE)
data_list = CSV_files |>
lapply(fread) |>
setNames(gsub("(_data)?.csv", "", CSV_files))
cleaned_data_list = copy(data_list)
names(cleaned_data_list)
head(cleaned_data_list[["laws_incentives"]])
cleaned_data_list[["gasprice"]] =
cleaned_data_list[["gasprice"]] |>
set(j = "date", value = NULL) |>
ts(frequency = 12, start = c(2000, 6)) |>
window(start = c(2012, 1), end = c(2022, 12)) |>
aggregate(FUN = mean, na.rm = TRUE) |>
t() |>
as.data.table(keep.rownames = TRUE) |>
setNames(c("state", paste0("avg_gasprice_", as.character(2012:2022))))
here::i_am(".git/config")
knitr::opts_knit$set("root.dir" = here::here("data_aggregate"))
setDTthreads(threads = 0)
state.name = c(state.name, "District of Columbia")
state.abb = c(state.abb, "DC")
formals(merge.data.table)$all = TRUE
merge = merge.data.table
getwd()
setwd(here::i_am(".git/config"))
here::i_am(".git/config")
setwd("/home/david/Desktop/Homework Scans/2023S_SDS425/data_aggregate/class_datasets")
library(tidyr)
library(stringr)
library(magrittr)
library(lubridate)
library(data.table)
library(collapse)
library(ggplot2)
library(knitr)
library(here)
here::i_am(".git/config")
knitr::opts_knit$set("root.dir" = here::here("data_aggregate"))
setDTthreads(threads = 0)
state.name = c(state.name, "District of Columbia")
state.abb = c(state.abb, "DC")
formals(merge.data.table)$all = TRUE
merge = merge.data.table
CSV_files = grep(".csv", list.files(), value = TRUE)
data_list = CSV_files |>
lapply(fread) |>
setNames(gsub("(_data)?.csv", "", CSV_files))
cleaned_data_list = copy(data_list)
names(cleaned_data_list)
head(cleaned_data_list[["laws_incentives"]])
cleaned_data_list[["gasprice"]] =
cleaned_data_list[["gasprice"]] |>
set(j = "date", value = NULL) |>
ts(frequency = 12, start = c(2000, 6)) |>
window(start = c(2012, 1), end = c(2022, 12)) |>
aggregate(FUN = mean, na.rm = TRUE) |>
t() |>
as.data.table(keep.rownames = TRUE) |>
setNames(c("state", paste0("avg_gasprice_", as.character(2012:2022))))
setwd("/home/david/Desktop/Homework Scans/2023S_SDS425/data_aggregate/class_datasets")
CSV_files = grep(".csv", list.files(), value = TRUE)
data_list = CSV_files |>
lapply(fread) |>
setNames(gsub("(_data)?.csv", "", CSV_files))
cleaned_data_list = copy(data_list)
names(cleaned_data_list)
grep(".csv", list.files(), value = TRUE)
setwd("/home/david/Desktop/Homework Scans/2023S_SDS425/data_aggregate/class_datasets")
setwd("/home/david/Desktop/Homework Scans/2023S_SDS425/data_aggregate/class_datasets")
CSV_files = grep(".csv", list.files(), value = TRUE)
data_list = CSV_files |>
lapply(fread) |>
setNames(gsub("(_data)?.csv", "", CSV_files))
cleaned_data_list = copy(data_list)
names(cleaned_data_list)
head(cleaned_data_list[["laws_incentives"]])
cleaned_data_list[["gasprice"]] =
cleaned_data_list[["gasprice"]] |>
set(j = "date", value = NULL) |>
ts(frequency = 12, start = c(2000, 6)) |>
window(start = c(2012, 1), end = c(2022, 12)) |>
aggregate(FUN = mean, na.rm = TRUE) |>
t() |>
as.data.table(keep.rownames = TRUE) |>
setNames(c("state", paste0("avg_gasprice_", as.character(2012:2022))))
cleaned_data_list[["gasprice"]][, state := state.abb[match(state, gsub("[[:space:]]", "", tolower(state.name)))]]
head(cleaned_data_list[["gasprice"]])
cleaned_data_list[["climate_attitudes"]][, (5:19) := lapply(.SD, as.numeric), .SDcols = -c(1:4)]
cleaned_data_list[["climate_attitudes"]][, state := state.abb[match(trimws(State), c(state.name))]]
cleaned_data_list[["climate_attitudes"]][, c("STATEFP", "COUNTYFP") := GEOID |>
str_pad(5, pad = "0") |>
substring(first = c(1,3), last = c(2,5)) |>
as.integer() |>
as.list(), by = GEOID]
cleaned_data_list[["climate_attitudes"]][, c("GEOID", "State", "County", "GeoName") := NULL]
cleaned_data_list[["climate_attitudes"]] = unique(cleaned_data_list[["climate_attitudes"]], by = c("STATEFP", "COUNTYFP"), fromLast = TRUE)
head(cleaned_data_list[["climate_attitudes"]])
cleaned_data_list[["election"]][, `:=`(proportion = candidatevotes/totalvotes,
party = substring(tolower(party), 1, 3))]
cleaned_data_list[["election"]][, c("STATEFP", "COUNTYFP") := county_fips |>
str_pad(5, pad = "0") |>
substring(first = c(1,3), last = c(2,5)) |>
as.integer() |>
as.list(), by = county_fips]
cleaned_data_list[["election"]] = cleaned_data_list[["election"]] |>
subset(party == "dem") |>
set(j = c("state", "county_name", "county_fips", "party", "candidatevotes", "totalvotes"), value = NULL) |>
dplyr::distinct() |>
na.omit() |>
dcast(... ~ year, value.var = "proportion", sep = "_dem_proportion", fun.aggregate = sum) |>
setnames(old = c("state_po", "2016", "2020"), new = c("state", "2016_dem_proportion", "2020_dem_proportion"))
head(cleaned_data_list[["election"]])
merged_data = with(cleaned_data_list, {
state_merged_data = Reduce(merge, list(laws_incentives, gasprice))
county_merged_data = Reduce(merge, list(election, climate_attitudes))
GEOID_merged_data = Reduce(merge, list(POI_counts, road_mileage, rest_stops))
state_merged_data[
county_merged_data[
GEOID_merged_data[
EVstations[census, on = .NATURAL] |> replace_NA(value = 0, cols = names(EVstations)),
on = "GEOID"],
on = c("state", "STATEFP", "COUNTYFP")],
on = "state"]
})
cleaned_data_list[["EVstations"]] = with(cleaned_data_list, EVstations[EVstations_coord_GEOID, on = .NATURAL])
cleaned_data_list[["EVstations"]] = cleaned_data_list[["EVstations"]][!is.na(GEOID), c(.N, lapply(.SD, sum, na.rm = TRUE)), by=GEOID, .SDcols = 18:20] |> setnames("N", "N.Stations")
top_50_categories = cleaned_data_list[["POI_counts"]][, colSums(.SD, na.rm = TRUE), .SDcols = -"GEOID"] |>
sort() |>
rev() |>
names() |>
head(50)
cleaned_data_list[["POI_counts"]] = cleaned_data_list[["POI_counts"]][, .SD, .SDcols = c("GEOID", top_50_categories)]
head(cleaned_data_list[["POI_counts"]])
head(cleaned_data_list[["rest_stops"]])
head(cleaned_data_list[["road_mileage"]])
head(cleaned_data_list[["road_mileage"]])
top_50_categories = cleaned_data_list[["POI_counts"]][, colSums(.SD, na.rm = TRUE), .SDcols = -"GEOID"] |>
sort() |>
rev() |>
names() |>
head(50)
cleaned_data_list[["POI_counts"]] = cleaned_data_list[["POI_counts"]][, .SD, .SDcols = c("GEOID", top_50_categories)]
head(cleaned_data_list[["POI_counts"]])
top_50_categories
cleaned_data_list[["EVstations"]] = with(cleaned_data_list, EVstations[EVstations_coord_GEOID, on = .NATURAL])
cleaned_data_list[["EVstations"]] = cleaned_data_list[["EVstations"]][!is.na(GEOID), c(.N, lapply(.SD, sum, na.rm = TRUE)), by=GEOID, .SDcols = 18:20] |> setnames("N", "N.Stations")
library(tidyr)
library(stringr)
library(magrittr)
library(lubridate)
library(data.table)
library(collapse)
library(ggplot2)
library(knitr)
library(here)
here::i_am(".git/config")
knitr::opts_knit$set("root.dir" = here::here("data_aggregate"))
setDTthreads(threads = 0)
state.name = c(state.name, "District of Columbia")
state.abb = c(state.abb, "DC")
formals(merge.data.table)$all = TRUE
merge = merge.data.table
setwd("/home/david/Desktop/Homework Scans/2023S_SDS425/data_aggregate/class_datasets")
CSV_files = grep(".csv", list.files(), value = TRUE)
data_list = CSV_files |>
lapply(fread) |>
setNames(gsub("(_data)?.csv", "", CSV_files))
cleaned_data_list = copy(data_list)
names(cleaned_data_list)
head(cleaned_data_list[["laws_incentives"]])
cleaned_data_list[["gasprice"]] =
cleaned_data_list[["gasprice"]] |>
set(j = "date", value = NULL) |>
ts(frequency = 12, start = c(2000, 6)) |>
window(start = c(2012, 1), end = c(2022, 12)) |>
aggregate(FUN = mean, na.rm = TRUE) |>
t() |>
as.data.table(keep.rownames = TRUE) |>
setNames(c("state", paste0("avg_gasprice_", as.character(2012:2022))))
cleaned_data_list[["gasprice"]][, state := state.abb[match(state, gsub("[[:space:]]", "", tolower(state.name)))]]
head(cleaned_data_list[["gasprice"]])
cleaned_data_list[["climate_attitudes"]][, (5:19) := lapply(.SD, as.numeric), .SDcols = -c(1:4)]
cleaned_data_list[["climate_attitudes"]][, state := state.abb[match(trimws(State), c(state.name))]]
cleaned_data_list[["climate_attitudes"]][, c("STATEFP", "COUNTYFP") := GEOID |>
str_pad(5, pad = "0") |>
substring(first = c(1,3), last = c(2,5)) |>
as.integer() |>
as.list(), by = GEOID]
cleaned_data_list[["climate_attitudes"]][, c("GEOID", "State", "County", "GeoName") := NULL]
cleaned_data_list[["climate_attitudes"]] = unique(cleaned_data_list[["climate_attitudes"]], by = c("STATEFP", "COUNTYFP"), fromLast = TRUE)
head(cleaned_data_list[["climate_attitudes"]])
cleaned_data_list[["election"]][, `:=`(proportion = candidatevotes/totalvotes,
party = substring(tolower(party), 1, 3))]
cleaned_data_list[["election"]][, c("STATEFP", "COUNTYFP") := county_fips |>
str_pad(5, pad = "0") |>
substring(first = c(1,3), last = c(2,5)) |>
as.integer() |>
as.list(), by = county_fips]
cleaned_data_list[["election"]] = cleaned_data_list[["election"]] |>
subset(party == "dem") |>
set(j = c("state", "county_name", "county_fips", "party", "candidatevotes", "totalvotes"), value = NULL) |>
dplyr::distinct() |>
na.omit() |>
dcast(... ~ year, value.var = "proportion", sep = "_dem_proportion", fun.aggregate = sum) |>
setnames(old = c("state_po", "2016", "2020"), new = c("state", "2016_dem_proportion", "2020_dem_proportion"))
head(cleaned_data_list[["election"]])
head(cleaned_data_list[["road_mileage"]])
head(cleaned_data_list[["rest_stops"]])
top_50_categories = cleaned_data_list[["POI_counts"]][, colSums(.SD, na.rm = TRUE), .SDcols = -"GEOID"] |>
sort() |>
rev() |>
names() |>
head(50)
cleaned_data_list[["POI_counts"]] = cleaned_data_list[["POI_counts"]][, .SD, .SDcols = c("GEOID", top_50_categories)]
head(cleaned_data_list[["POI_counts"]])
cleaned_data_list[["EVstations"]] = with(cleaned_data_list, EVstations[EVstations_coord_GEOID, on = .NATURAL])
cleaned_data_list[["EVstations"]] = cleaned_data_list[["EVstations"]][!is.na(GEOID), c(.N, lapply(.SD, sum, na.rm = TRUE)), by=GEOID, .SDcols = 18:20] |> setnames("N", "N.Stations")
merged_data = with(cleaned_data_list, {
state_merged_data = Reduce(merge, list(laws_incentives, gasprice))
county_merged_data = Reduce(merge, list(election, climate_attitudes))
GEOID_merged_data = Reduce(merge, list(POI_counts, road_mileage, rest_stops))
state_merged_data[
county_merged_data[
GEOID_merged_data[
EVstations[census, on = .NATURAL] |> replace_NA(value = 0, cols = names(EVstations)),
on = "GEOID"],
on = c("state", "STATEFP", "COUNTYFP")],
on = "state"]
})
merged_data[, `:=`(STATEFP = str_pad(STATEFP, 2, pad = 0),
COUNTYFP = str_pad(COUNTYFP, 3, pad = 0),
TRACTCE = str_pad(TRACTCE, 6, pad = 0),
GEOID = str_pad(GEOID, 11, pad = 0))] |>
setcolorder(c("GEOID", "state", "STATEFP", "COUNTYFP", "TRACTCE"))
head(merged_data)
fwrite(merged_data, "../merged_data.csv")
EVstations_coord_GEOID
cleaned_data_list$EVstations_coord_GEOID
cleaned_data_list$EVstations
asdf= fread("EVstations.csv")
asdf
asdf[!is.na(GEOID), .N, by=GEOID, .SDcols = 18:20]
asdf
asdf2 = with(cleaned_data_list, asdf[EVstations_coord_GEOID, on = .NATURAL])
asdf2[!is.na(GEOID), c(.N, .SD)), by=GEOID, .SDcols = 18:20]
asdf2[!is.na(GEOID), c(.N, .SD), by=GEOID, .SDcols = 18:20]
asdf2[!is.na(GEOID), .SD, by=GEOID, .SDcols = 18:20]
cleaned_data_list[["EVstations"]] = with(cleaned_data_list, EVstations[EVstations_coord_GEOID, on = .NATURAL])
cleaned_data_list[["EVstations"]] = cleaned_data_list[["EVstations"]][!is.na(GEOID), c(.N, lapply(.SD, sum, na.rm = TRUE)), by=GEOID, .SDcols = 18:20] |> setnames("N", "N.Stations")
cleaned_data_list[["EVstations"]] = with(cleaned_data_list, EVstations[EVstations_coord_GEOID, on = .NATURAL])
cleaned_data_list[["EVstations"]] = cleaned_data_list[["EVstations"]][!is.na(GEOID), c(.N, lapply(.SD, sum, na.rm = TRUE)), by=GEOID, .SDcols = 18:20] |> setnames("N", "N.Stations")
library(tidyr)
library(stringr)
library(magrittr)
library(lubridate)
library(data.table)
library(collapse)
library(ggplot2)
library(knitr)
library(here)
here::i_am(".git/config")
knitr::opts_knit$set("root.dir" = here::here("data_aggregate"))
setDTthreads(threads = 0)
state.name = c(state.name, "District of Columbia")
state.abb = c(state.abb, "DC")
formals(merge.data.table)$all = TRUE
merge = merge.data.table
setwd("/home/david/Desktop/Homework Scans/2023S_SDS425/data_aggregate/class_datasets")
CSV_files = grep(".csv", list.files(), value = TRUE)
data_list = CSV_files |>
lapply(fread) |>
setNames(gsub("(_data)?.csv", "", CSV_files))
cleaned_data_list = copy(data_list)
names(cleaned_data_list)
head(cleaned_data_list[["laws_incentives"]])
cleaned_data_list[["gasprice"]] =
cleaned_data_list[["gasprice"]] |>
set(j = "date", value = NULL) |>
ts(frequency = 12, start = c(2000, 6)) |>
window(start = c(2012, 1), end = c(2022, 12)) |>
aggregate(FUN = mean, na.rm = TRUE) |>
t() |>
as.data.table(keep.rownames = TRUE) |>
setNames(c("state", paste0("avg_gasprice_", as.character(2012:2022))))
cleaned_data_list[["gasprice"]][, state := state.abb[match(state, gsub("[[:space:]]", "", tolower(state.name)))]]
head(cleaned_data_list[["gasprice"]])
cleaned_data_list[["climate_attitudes"]][, (5:19) := lapply(.SD, as.numeric), .SDcols = -c(1:4)]
cleaned_data_list[["climate_attitudes"]][, state := state.abb[match(trimws(State), c(state.name))]]
cleaned_data_list[["climate_attitudes"]][, c("STATEFP", "COUNTYFP") := GEOID |>
str_pad(5, pad = "0") |>
substring(first = c(1,3), last = c(2,5)) |>
as.integer() |>
as.list(), by = GEOID]
cleaned_data_list[["climate_attitudes"]][, c("GEOID", "State", "County", "GeoName") := NULL]
cleaned_data_list[["climate_attitudes"]] = unique(cleaned_data_list[["climate_attitudes"]], by = c("STATEFP", "COUNTYFP"), fromLast = TRUE)
head(cleaned_data_list[["climate_attitudes"]])
cleaned_data_list[["election"]][, `:=`(proportion = candidatevotes/totalvotes,
party = substring(tolower(party), 1, 3))]
cleaned_data_list[["election"]][, c("STATEFP", "COUNTYFP") := county_fips |>
str_pad(5, pad = "0") |>
substring(first = c(1,3), last = c(2,5)) |>
as.integer() |>
as.list(), by = county_fips]
cleaned_data_list[["election"]] = cleaned_data_list[["election"]] |>
subset(party == "dem") |>
set(j = c("state", "county_name", "county_fips", "party", "candidatevotes", "totalvotes"), value = NULL) |>
dplyr::distinct() |>
na.omit() |>
dcast(... ~ year, value.var = "proportion", sep = "_dem_proportion", fun.aggregate = sum) |>
setnames(old = c("state_po", "2016", "2020"), new = c("state", "2016_dem_proportion", "2020_dem_proportion"))
head(cleaned_data_list[["election"]])
head(cleaned_data_list[["road_mileage"]])
head(cleaned_data_list[["rest_stops"]])
top_50_categories = cleaned_data_list[["POI_counts"]][, colSums(.SD, na.rm = TRUE), .SDcols = -"GEOID"] |>
sort() |>
rev() |>
names() |>
head(50)
cleaned_data_list[["POI_counts"]] = cleaned_data_list[["POI_counts"]][, .SD, .SDcols = c("GEOID", top_50_categories)]
head(cleaned_data_list[["POI_counts"]])
cleaned_data_list[["EVstations"]] = with(cleaned_data_list, EVstations[EVstations_coord_GEOID, on = .NATURAL])
cleaned_data_list[["EVstations"]] = cleaned_data_list[["EVstations"]][!is.na(GEOID), c(.N, lapply(.SD, sum, na.rm = TRUE)), by=GEOID, .SDcols = 18:20] |> setnames("N", "N.Stations")
merged_data = with(cleaned_data_list, {
state_merged_data = Reduce(merge, list(laws_incentives, gasprice))
county_merged_data = Reduce(merge, list(election, climate_attitudes))
GEOID_merged_data = Reduce(merge, list(POI_counts, road_mileage, rest_stops))
state_merged_data[
county_merged_data[
GEOID_merged_data[
EVstations[census, on = .NATURAL] |> replace_NA(value = 0, cols = names(EVstations)),
on = "GEOID"],
on = c("state", "STATEFP", "COUNTYFP")],
on = "state"]
})
merged_data[, `:=`(STATEFP = str_pad(STATEFP, 2, pad = 0),
COUNTYFP = str_pad(COUNTYFP, 3, pad = 0),
TRACTCE = str_pad(TRACTCE, 6, pad = 0),
GEOID = str_pad(GEOID, 11, pad = 0))] |>
setcolorder(c("GEOID", "state", "STATEFP", "COUNTYFP", "TRACTCE"))
head(merged_data)
fwrite(merged_data, "../merged_data.csv")
head(cleaned_data_list[["EVstations"]])
EVstations[census, on = .NATURAL]
cleaned_data_list$EVstations[cleaned_data_list$census, on = .NATURAL]
asdf = cleaned_data_list$EVstations[cleaned_data_list$census, on = .NATURAL]
View(asdf)
View(asdf)
library(broom)
library(corrplot)
library(GGally)
library(ggplot2)
library(sf)
library(glmnet)
library(pscl)
library(data.table)
library(collapse)
library(doFuture)
library(parallel)
library(knitr)
library(here)
here::i_am(".git/config")
knitr::opts_knit$set("root.dir" = here::here("data_analyze"))
data_full = fread("../data_aggregate/merged_data.csv")
setDTthreads(threads = 0)
plan(multisession)
doFuture::registerDoFuture()
library(broom)
library(corrplot)
library(GGally)
library(ggplot2)
library(sf)
library(glmnet)
library(pscl)
library(data.table)
library(collapse)
library(doFuture)
library(parallel)
library(knitr)
library(here)
here::i_am(".git/config")
knitr::opts_knit$set("root.dir" = here::here("data_analyze"))
data_full = fread("../data_aggregate/merged_data.csv")
setDTthreads(threads = 0)
plan(multisession)
doFuture::registerDoFuture()
col_NA_counts =
t(data_full[, lapply(.SD, purrr::compose(sum, is.na))]) %>%
data.table(keep.rownames = "var") %>%
setorderv("V1", order = -1) %>%
with(setNames(split(var, group(V1)), paste0("#NA = ", unique(V1))))
data_NA.free = na.omit(data_full[, .SD, .SDcols = -unlist(col_NA_counts[1:2])])
head(col_NA_counts, 3)
cat("nrow full merged dataset ", nrow(data_full))
cat("\n")
cat("\nnrow NA.free dataset ", nrow(data_NA.free))
data_POI_PCA =
copy(data_NA.free) |>
subset(select = col_NA_counts[[3]]) |>
prcomp()
setDT(data_POI_PCA["sdev"])[,c("idx", "variance") := .(.I, sdev^2)] %>%
qplot(x = idx, y = variance, geom = c("line", "point"), data = .)
cat("By keeping the POI data's first", which.min(c(summary(data_POI_PCA)[[6]][3,]) < 0.9), "Principal Components we retain over 90% of the entire dataset's variance")
data_POI_PCA = cbind(copy(data_NA.free)[, .SD, .SDcols = -col_NA_counts[[3]]], data_POI_PCA$x[,1:10])
good_data_x = copy(data_POI_PCA)[, !data_POI_PCA[, names(data_POI_PCA) %like% "EV|N.Stations"
| sapply(.SD, is.character)], with = FALSE]
good_data_y = data_POI_PCA$N.Stations
cv_tune.lasso_model = suppressMessages(suppressWarnings(
cv.glmnet(x = data.matrix(good_data_x),
y = good_data_y,
nlambda = 1000,
nfolds = 500,
pmax = 15,
parallel = TRUE)))
plot(cv_tune.lasso_model)
lasso_model = glmnet(x = good_data_x, y = good_data_y, lambda =  cv_tune.lasso_model$lambda.min)
coef(lasso_model)
vars_keep = rownames(coef(lasso_model))[which(as.matrix(coef(lasso_model)) != 0)][-1]
data_keep = copy(data_POI_PCA)[, ..vars_keep]
head(data_keep)
corrplot.mixed(
cor(data_keep),
lower.col = "black",
upper = "ellipse",
number.cex = .25,
tl.col = "black",
tl.pos = "lt",
tl.cex = .5
)
GGally::ggpairs(dplyr::sample_n(data_keep, 100)) +
theme(
plot.title = element_text(face = 'bold',
size = 10,
hjust = 0.5, margin = margin(b = 20)),
axis.text.x = element_text(angle = 45, hjust = 1)
)
lm_model = lm(good_data_y ~ data.matrix(data_keep))
summary(lm_model)
plot(lm_model)
logit_model = glm(as.logical(good_data_y) ~ data.matrix(data_keep), family = binomial(link = "logit"))
summary(logit_model)
plot(logit_model)
data_keep_int = data_keep[, data_keep[, !sapply(.SD, is.double)], with = FALSE]
zip_model = pscl::zeroinfl(good_data_y ~ data.matrix(data_keep_int), dist = "negbin")
summary(zip_model)
col_NA_counts
data_POI_PCA
copy(data_NA.free) |>
subset(select = col_NA_counts[[3]]) |>
prcomp(scale = TRUE) |> summary()
copy(data_NA.free) |>
subset(select = col_NA_counts[[3]]) |>
prcomp(scale = TRUE)
copy(data_NA.free) |>
subset(select = col_NA_counts[[3]]) |>
prcomp(scale = TRUE)  |>getElement("PC1")
copy(data_NA.free) |>
subset(select = col_NA_counts[[3]]) |>
prcomp(scale = TRUE) |> subset(1)
copy(data_NA.free) |>
subset(select = col_NA_counts[[3]]) |>
prcomp(scale = TRUE) -> asdf
asdf[,1]
asdf[[1]]
asdf$rotation[,1]
cor(asdf$rotation[,1] )
install.packages("library(corrplot)")
install.packages("corrplot")
install.packages("corrplot")
library(corrplot)
corrplot(asdf$rotation[,1], type = "upper", order = "hclust",
tl.col = "black", tl.srt = 45)
corrplot(as.matrix(asdf$rotation[,1]), type = "upper", order = "hclust",
tl.col = "black", tl.srt = 45)
cor(asdf$rotation[,1])
cor(c(asdf$rotation[,1])
)
head(asdf$rotation[,1])
class(asdf$rotation[,1])
class(as.matrix(asdf$rotation[,1]))
corr(as.matrix(asdf$rotation[,1]))
cor(as.matrix(asdf$rotation[,1]))
asdf$rotation[,1]
cor(matrix(asdf$rotation[,1]))
cor(matrix(asdf$rotation))
matrix(asdf$rotation)
as.matrix(asdf$rotation[,1])
as.matrix(asdf$rotation)
cor(as.matrix(asdf$rotation))
cor(t(as.matrix(asdf$rotation)))
corrplot(cor(t(as.matrix(asdf$rotation))), type = "upper", order = "hclust",
tl.col = "black", tl.srt = 45)
corrplot(cor(t(as.matrix(asdf$rotation))))
corrplot.mixed(
cor(t(as.matrix(asdf$rotation))),
lower.col = "black",
upper = "ellipse",
number.cex = .25,
tl.col = "black",
tl.pos = "lt",
tl.cex = .5
)
asdf$rotation
View(good_data_x)
View(good_data_x)
names(data_POI_PCA)
clear
rep(1:3, nrow(data_keep) % 3)
rep(1:3, nrow(data_keep) %/% 3)
